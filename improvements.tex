% Conclusions and future work


\chapter{Conclusions and future work}
\label{chap:conclusions-and-future-work}
This chapter draws some conclusions and describes some of the improvements that can be applied to our High Availability system. These improvements can be used in further research.

\section {Conclusions}

We have demonstrated thanks to a reproducible example that Zimbra OSE could be enhaced to have high availability by the means of a system based on state of the art open source high availability software such as Pacemaker and Corosync.

The master thesis writer has learnt how to develop OCF Resource Agents which can be useful for other high availability systems. How to mirror a partition between two servers thanks to DRBD was also covered. More than that, Zimbra, one of the easiest email server solutions in its open source edition can now be used as a high availability system with standard high availability software.

\section {Future work}

\subsection {OVH Datacentre network handling}
There have been some efforts from the bTactic team to handle OVH Datacentre networking thanks to three OCF scripts named: 
ClusterOVHFailover, ClusterHostRoute and ClusterDefaultRoute. These scripts make sense in setup that doesn't use Virtual Rack + RIPE but just normal servers connected via Internet only.

ClusterOVHFailover makes sure an OVH ip-fail-over is failed over from one machine to the another one. This way Zimbra Server is being served by the correct host.
ClusterHostRoute and ClusterDefaultRoute are meant to help to setup OVH networking for ip-fail-over which is not covered by standard network setup found in  default Pacemaker package.

These scripts can be found inside BtacticOCF tar.gz file (\cite{BtacticOCF}) which can be downloaded from BtacticOCF  tar.gz file (\cite{BtacticOrg}).

\subsection {\label{subsec:fencing}Fencing}
Fencing is the process of locking resources away from a node whose status is uncertain (\cite{LinuxHAFencing}). The default method of fencing in Pacemaker is using stonith. Stonith is a technique for node fencing, that means that the node (either primary or secondary server in our example) that it's supposed to have failed is \textit{shot in head} (\cite{LinuxHAStonith}). That makes sure that the node is actually dead.

In our described example we have disabled stonith. We can improve it by enabling it and using one of the available methods.

Once again bTactic team has developed an stonith script (or fence agent as known per Pacemaker) to make sure an OVH server doesn't use a shared resource. The failing node is rebooted into a Rescue mode which is quite similar to booting a computer with a live CD that doesn't make any change to local hard disks. Fence agent name is: fence\_ovh.

This script is available as a Red Hat's fence-agents package since fence-agents package 4.0.2 release version (\cite{LinuxClusterML201307}).

\subsection {\label{subsection:mysql-ha}Mysql HA}
One of the Zimbra components is a Mysql database. This Mysql database can be excluded from DRBD syncing and can be setup as an active / active cluster.

This setup will offer a better handling of node failing because you would only loose last mysql queries. In the DRBD case you loose all the file changes that have not been stored into the files that compose actual Mysql database.

The only drawback for this improvement is that Zimbra OSE upgrades tend to be much more difficult when we want to preserve high availability.

\subsection {Project Always ON}

Always ON is a project from Zimbra (\cite{ZimbraProjectAlwaysOn}) (September 2013) with a very simple goal: Email and collaboration should be \textit{always on} for end users. Its design goals are:
\begin{itemize}
  \item Inherently resilient to failure
  \item Scaling should be elastic based on workload demands
  \item The software can be enhanced without service disruption
  \item Efficient usage of commodity hardware resources
\end{itemize}
. That design goals imply:
\begin{itemize}
  \item No single points of failure in the application components
  \item Separating the application code from the data
  \item Distributing state information across commodity storage
  \item Automatic failover of application and data storage components
  \item Automatic load balancing of client requests across the application and data layers
\end{itemize}

Project Always ON has just started and might be implemented as early as in Zimbra 10 version. It's an improvement over Mysql HA future work described on subsection {\ref{subsection:mysql-ha} Mysql HA} because not only enforces high availability on each of the Zimbra components but it's also focused on scaling resources. That means, that the more service is needed the more Zimbra (virtual) machines will be deployed to meet that service requirements.

